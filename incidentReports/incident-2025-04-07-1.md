# Incident: 2025-04-07 08-40

## Summary

On April 7, 2025, the pizza ordering system broke. Customers couldn't buy pizzas because our system couldn't talk to the pizza factory service. I found the problem by running tests and fixed it by using a special repair link.

```md
**EXAMPLE**:

Between the hour of {time range of incident, e.g. 15:45 and 16:35} on {DATE}, {NUMBER} users encountered {EVENT SYMPTOMS}. The event was triggered by a {CHANGE} at {TIME OF CHANGE THAT CAUSED THE EVENT}. The {CHANGE} contained {DESCRIPTION OF OR REASON FOR THE CHANGE, such as a change in code to update a system}.

A bug in this code caused {DESCRIPTION OF THE PROBLEM}. The event was detected by {MONITORING SYSTEM}. The team started working on the event by {RESOLUTION ACTIONS TAKEN}. This {SEVERITY LEVEL} incident affected {X%} of users.

There was further impact as noted by {e.g. NUMBER OF SUPPORT TICKETS SUBMITTED, SOCIAL MEDIA MENTIONS, CALLS TO ACCOUNT MANAGERS} were raised in relation to this incident.
```

## Detection

I found the problem around 8:40 AM on April 7 when our test script showed errors. The test showed error codes 401 and 500 when trying to buy pizzas:
Bought a pizza... 401
Bought a pizza... 500
Also, on Grafana I set up metrics that alert when profit is 0 and when pizza sales were failing.

```md
**EXAMPLE**:

This incident was detected when the {ALERT TYPE} was triggered and {TEAM/PERSON} were paged.

Next, {SECONDARY PERSON} was paged, because {FIRST PERSON} didn't own the service writing to the disk, delaying the response by {XX MINUTES/HOURS}.

{DESCRIBE THE IMPROVEMENT} will be set up by {TEAM OWNER OF THE IMPROVEMENT} so that {EXPECTED IMPROVEMENT}.
```

## Impact

* What broke: Customers couldn't order pizzas
* Who was affected: All customers trying to buy pizzas
* Business impact:  couldn't sell any pizzas during this time

```md
**EXAMPLE**:

For {XXhrs XX minutes} between {XX:XX UTC and XX:XX UTC} on {MM/DD/YY}, {SUMMARY OF INCIDENT} our users experienced this incident.

This incident affected {XX} customers (X% OF {SYSTEM OR SERVICE} USERS), who experienced {DESCRIPTION OF SYMPTOMS}.

{XX NUMBER OF SUPPORT TICKETS AND XX NUMBER OF SOCIAL MEDIA POSTS} were submitted.
```

## Timeline

- _08:40_ - First errors appear in our logs
- _08:43_ - Noticed failed pizza orders in our tests
- _09:30_ - Confirmed orders were working again

```md
**EXAMPLE**:

All times are UTC.

- _11:48_ - K8S 1.9 upgrade of control plane is finished
- _12:46_ - Upgrade to V1.9 completed, including cluster-auto scaler and the BuildEng scheduler instance
- _14:20_ - Build Engineering reports a problem to the KITT Disturbed
- _14:27_ - KITT Disturbed starts investigating failures of a specific EC2 instance (ip-203-153-8-204)
- _14:42_ - KITT Disturbed cordons the node
- _14:49_ - BuildEng reports the problem as affecting more than just one node. 86 instances of the problem show failures are more systemic
- _15:00_ - KITT Disturbed suggests switching to the standard scheduler
- _15:34_ - BuildEng reports 200 pods failed
- _16:00_ - BuildEng kills all failed builds with OutOfCpu reports
- _16:13_ - BuildEng reports the failures are consistently recurring with new builds and were not just transient.
- _16:30_ - KITT recognize the failures as an incident and run it as an incident.
- _16:36_ - KITT disable the Escalator autoscaler to prevent the autoscaler from removing compute to alleviate the problem.
- _16:40_ - KITT confirms ASG is stable, cluster load is normal and customer impact resolved.
```

## Response

-I responded to the incident at approximately 8:43 AM on April 8, 2025, just a few minutes after errors began appearing in our logs. As soon as I noticed the failed pizza orders in our testing script, I immediately began investigating the issue.
-First, I checked the Grafana dashboard, which confirmed that our profit metrics had dropped to zero and pizza sales were failing. The alert I had previously set up for "profit = 0" had triggered, which helped validate that this was a real issue affecting our business operations.
-I checked all other metrics I had created and they showed that pizza purchases were failing.
```md
**EXAMPLE**:

After receiving a page at {XX:XX UTC}, {ON-CALL ENGINEER} came online at {XX:XX UTC} in {SYSTEM WHERE INCIDENT INFO IS CAPTURED}.

This engineer did not have a background in the {AFFECTED SYSTEM} so a second alert was sent at {XX:XX UTC} to {ESCALATIONS ON-CALL ENGINEER} into the who came into the room at {XX:XX UTC}.
```

## Root cause

- The problem was caused by a planned test called "chaos monkey" that broke the pizza factory service on purpose. The system couldn't order pizzas because the factory service was programmed to reject all requests.
- We found this by using a step-by-step troubleshooting approach:

* I pinged the factory service domain to check if it was reachable: pizza-factory.cs329.click
->The domain responded, confirming network connectivity was not the issue.
* I checked if the factory service was responding to basic requests: curl https://pizza-factory.cs329.click/api/health
The service responded with {"message":"unknown endpoint"}, showing the service was running but the endpoint didn't exist.
* We then tested the exact endpoint our application uses:
curl -X POST https://pizza-factory.cs329.click/api/order -H 'Content-Type: application/json' -H 'Authorization: Bearer myfactoryAPIKey' -d '{"test": true}'

This revealed the key error message:
{"message":"chaos monkey","reportUrl":"https://cs329.cs.byu.edu/api/report?apiKey=myfactoryAPIKey&fixCode=88563bf6b602499e9801c2ef8e19dcb5"}


```md
**EXAMPLE**:

A bug in connection pool handling led to leaked connections under failure conditions, combined with lack of visibility into connection state.
```

## Resolution

I fixed the problem by:

Using the repair link that was in the error message:
curl "https://cs329.cs.byu.edu/api/report?apiKey=myFactoryApiKey&fixCode=88563bf6b602499e9801c2ef8e19dcb5"

The system told us it was fixed:
{"msg":"Chaos resolved"}

We confirmed orders were working again by running our tests.
```md
**EXAMPLE**:
By Increasing the size of the BuildEng EC3 ASG to increase the number of nodes available to support the workload and reduce the likelihood of scheduling on oversubscribed nodes

Disabled the Escalator autoscaler to prevent the cluster from aggressively scaling-down
Reverting the Build Engineering scheduler to the previous version.
```

## Prevention

-Since this was for an assignment, and I was able to check the terminal constantly during the test hours, I caught the problem right when the chaos started. We were told that we didn't have to change our code and the hint was "URL," which helped to narrow down what could be wrong. But in real life, it wouldn't be this simple.
-I could add more specific alerts for certain log patterns by thinking about what could go wrong, so that we can get alerts without having to watch the terminal. I could also create a troubleshooting guide with steps to follow when specific problems happen.

```md
**EXAMPLE**:

This same root cause resulted in incidents HOT-13432, HOT-14932 and HOT-19452.
```

## Action items

-Improve Monitoring System
-Create Factory Service Troubleshooting Guide
-Regular Testing Schedule

```md
**EXAMPLE**:

1. Manual auto-scaling rate limit put in place temporarily to limit failures
1. Unit test and re-introduction of job rate limiting
1. Introduction of a secondary mechanism to collect distributed rate information across cluster to guide scaling effects
```
